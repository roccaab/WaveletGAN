{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyMQt/GckATEFANXtWIlGDq9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roccaab/WaveletGAN/blob/main/Tesi_Presentazione.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1KThTqUcQLn9",
        "outputId": "74cc2506-9451-474d-cc3c-73287480232e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pywavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from pywavelets) (2.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: pywavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Dispositivo utilizzato: cpu\n",
            "Dispositivo utilizzato: cpu\n",
            "Per favore, carica il file CSV contenente i dati degli eventi sismici reali.\n",
            "Il file deve contenere le colonne: esm_event_id, station_code, mw, epi_dist\n",
            "Formato CSV con separatore punto e virgola (;)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c62e4d68-862a-4311-bb69-3249cca52d40\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c62e4d68-862a-4311-bb69-3249cca52d40\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dati.csv to dati.csv\n",
            "File CSV caricato con successo: dati.csv\n",
            "Il file contiene 34 eventi sismici.\n",
            "Prime righe del file:\n",
            "            esm_event_id           event_time  ingv_event_id  ev_latitude  \\\n",
            "0  EMSC-20161030_0000029  2016-10-30T06:40:18      8863681.0     42.83794   \n",
            "1  EMSC-20161030_0000029  2016-10-30T06:40:18      8863681.0     42.83794   \n",
            "2  EMSC-20161030_0000029  2016-10-30T06:40:18      8863681.0     42.83794   \n",
            "3  EMSC-20161030_0000029  2016-10-30T06:40:18      8863681.0     42.83794   \n",
            "4  EMSC-20161030_0000029  2016-10-30T06:40:18      8863681.0     42.83794   \n",
            "\n",
            "   ev_longitude  ev_depth_km              ev_hyp_ref fm_type_code  \\\n",
            "0      13.12324        6.169  Spallarossa_et_al_2021           NF   \n",
            "1      13.12324        6.169  Spallarossa_et_al_2021           NF   \n",
            "2      13.12324        6.169  Spallarossa_et_al_2021           NF   \n",
            "3      13.12324        6.169  Spallarossa_et_al_2021           NF   \n",
            "4      13.12324        6.169  Spallarossa_et_al_2021           NF   \n",
            "\n",
            "             fm_ref    ml  ... w_t0_050  w_t0_100 w_t0_200  w_t0_300 w_t0_500  \\\n",
            "0  Pizzi_et_el_2017  5.51  ...  491.411   892.544  864.025  1531.800  739.096   \n",
            "1  Pizzi_et_el_2017  5.51  ...  543.728   574.516  685.965   767.735  677.206   \n",
            "2  Pizzi_et_el_2017  5.51  ...  405.179   661.095  362.875   317.720  203.059   \n",
            "3  Pizzi_et_el_2017  5.51  ...  438.012   853.277  573.021   513.713  550.706   \n",
            "4  Pizzi_et_el_2017  5.51  ...  138.200   183.834  200.934   131.955  143.847   \n",
            "\n",
            "   w_t1_000  w_t2_000  w_t3_000 w_t5_000          original_data_mediator  \n",
            "0   311.511  146.4900  120.2070  26.6703  http://webservices.rm.ingv.it/  \n",
            "1   204.286   66.7451   54.0064  13.6190  http://webservices.rm.ingv.it/  \n",
            "2   205.031   72.0732   49.4054  15.2025  http://webservices.rm.ingv.it/  \n",
            "3   245.224  188.3140  107.3690  47.5721  http://webservices.rm.ingv.it/  \n",
            "4   161.628   89.8675   66.5488  14.7385  http://webservices.rm.ingv.it/  \n",
            "\n",
            "[5 rows x 86 columns]\n",
            "\n",
            "====== FASE 1: Download e preprocessamento dei dati ======\n",
            "Colonne disponibili nel CSV: ['esm_event_id', 'event_time', 'ingv_event_id', 'ev_latitude', 'ev_longitude', 'ev_depth_km', 'ev_hyp_ref', 'fm_type_code', 'fm_ref', 'ml', 'ml_ref', 'mw', 'mw_ref', 'network_code', 'station_code', 'location_code', 'channel_code', 'sensor_depth_m', 'proximity', 'housing', 'installation', 'st_latitude', 'st_longitude', 'st_elevation', 'reference_site', 'preferred_estimation_method_vs30_ec8', 'preferred_ec8_code', 'preferred_vs30_m_s', 'epi_dist', 'epi_az', 'jb_dist', 'rup_dist', 'late_triggered_event_01', 'u_channel_code', 'v_channel_code', 'w_channel_code', 'processing_type', 'u_un_pga', 'v_un_pga', 'w_un_pga', 'u_hp', 'v_hp', 'w_hp', 'u_lp', 'v_lp', 'w_lp', 'u_pga', 'v_pga', 'w_pga', 'u_pgv', 'v_pgv', 'w_pgv', 'u_t90', 'v_t90', 'w_t90', 'u_ia', 'v_ia', 'w_ia', 'u_t0_050', 'u_t0_100', 'u_t0_200', 'u_t0_300', 'u_t0_500', 'u_t1_000', 'u_t2_000', 'u_t3_000', 'u_t5_000', 'v_t0_050', 'v_t0_100', 'v_t0_200', 'v_t0_300', 'v_t0_500', 'v_t1_000', 'v_t2_000', 'v_t3_000', 'v_t5_000', 'w_t0_050', 'w_t0_100', 'w_t0_200', 'w_t0_300', 'w_t0_500', 'w_t1_000', 'w_t2_000', 'w_t3_000', 'w_t5_000', 'original_data_mediator']\n",
            "Trovati 34 eventi nel file CSV\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ04 (1/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ04.zip\n",
            "Elaborazione file 3A.MZ04..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=1\n",
            "Trovato PGA nei metadati: 633.770447 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ04..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=1, componente E, PGA=633.770447\n",
            "Elaborazione file 3A.MZ04..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=2\n",
            "Trovato PGA nei metadati: 793.285278 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ04..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=2, componente N, PGA=793.285278\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ08 (2/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ08.zip\n",
            "Elaborazione file 3A.MZ08..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=3\n",
            "Trovato PGA nei metadati: 526.746216 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ08..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=3, componente E, PGA=526.746216\n",
            "Elaborazione file 3A.MZ08..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=4\n",
            "Trovato PGA nei metadati: 427.39151 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ08..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=4, componente N, PGA=427.39151\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ10 (3/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ10.zip\n",
            "Elaborazione file 3A.MZ10..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=5\n",
            "Trovato PGA nei metadati: 312.852783 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ10..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=5, componente E, PGA=312.852783\n",
            "Elaborazione file 3A.MZ10..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=6\n",
            "Trovato PGA nei metadati: 390.014984 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ10..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=6, componente N, PGA=390.014984\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ102 (4/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ102.zip\n",
            "Elaborazione file 3A.MZ102..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=7\n",
            "Trovato PGA nei metadati: 365.203674 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ102..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=7, componente E, PGA=365.203674\n",
            "Elaborazione file 3A.MZ102..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=8\n",
            "Trovato PGA nei metadati: 397.232544 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ102..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=8, componente N, PGA=397.232544\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ11 (5/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ11.zip\n",
            "Elaborazione file 3A.MZ11..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=9\n",
            "Trovato PGA nei metadati: 165.739304 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ11..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=9, componente E, PGA=165.739304\n",
            "Elaborazione file 3A.MZ11..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=10\n",
            "Trovato PGA nei metadati: 195.108139 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ11..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=10, componente N, PGA=195.108139\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ12 (6/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ12.zip\n",
            "Elaborazione file 3A.MZ12..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=11\n",
            "Trovato PGA nei metadati: 707.006068 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ12..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=11, componente E, PGA=707.006068\n",
            "Elaborazione file 3A.MZ12..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=12\n",
            "Trovato PGA nei metadati: 672.931507 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ12..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=12, componente N, PGA=672.931507\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ14 (7/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ14.zip\n",
            "Elaborazione file 3A.MZ14..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=13\n",
            "Trovato PGA nei metadati: 214.929199 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ14..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=13, componente E, PGA=214.929199\n",
            "Elaborazione file 3A.MZ14..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=14\n",
            "Trovato PGA nei metadati: 252.611206 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ14..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=14, componente N, PGA=252.611206\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ19 (8/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ19.zip\n",
            "Elaborazione file 3A.MZ19..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=15\n",
            "Trovato PGA nei metadati: 356.084717 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ19..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=15, componente E, PGA=356.084717\n",
            "Elaborazione file 3A.MZ19..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=16\n",
            "Trovato PGA nei metadati: 395.562622 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ19..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=16, componente N, PGA=395.562622\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ24 (9/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ24.zip\n",
            "Elaborazione file 3A.MZ24..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=17\n",
            "Trovato PGA nei metadati: 1001.65332 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ24..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=17, componente E, PGA=1001.65332\n",
            "Elaborazione file 3A.MZ24..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=18\n",
            "Trovato PGA nei metadati: 747.742981 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ24..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=18, componente N, PGA=747.742981\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ27 (10/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ27.zip\n",
            "Elaborazione file 3A.MZ27..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=19\n",
            "Trovato PGA nei metadati: 199.460495 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ27..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=19, componente E, PGA=199.460495\n",
            "Elaborazione file 3A.MZ27..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=20\n",
            "Trovato PGA nei metadati: 202.19516 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ27..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=20, componente N, PGA=202.19516\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ28 (11/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ28.zip\n",
            "Elaborazione file 3A.MZ28..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=21\n",
            "Trovato PGA nei metadati: 668.608587 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ28..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=21, componente E, PGA=668.608587\n",
            "Elaborazione file 3A.MZ28..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=22\n",
            "Trovato PGA nei metadati: 741.312207 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ28..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=22, componente N, PGA=741.312207\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ29 (12/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ29.zip\n",
            "Elaborazione file 3A.MZ29..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=23\n",
            "Trovato PGA nei metadati: 675.645813 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ29..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=23, componente E, PGA=675.645813\n",
            "Elaborazione file 3A.MZ29..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=24\n",
            "Trovato PGA nei metadati: 404.769806 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ29..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=24, componente N, PGA=404.769806\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ30 (13/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ30.zip\n",
            "Elaborazione file 3A.MZ30..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=25\n",
            "Trovato PGA nei metadati: 453.454254 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ30..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=25, componente E, PGA=453.454254\n",
            "Elaborazione file 3A.MZ30..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=26\n",
            "Trovato PGA nei metadati: 474.228638 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ30..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=26, componente N, PGA=474.228638\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ50 (14/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ50.zip\n",
            "Elaborazione file 3A.MZ50..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=27\n",
            "Trovato PGA nei metadati: 266.287292 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ50..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=27, componente E, PGA=266.287292\n",
            "Elaborazione file 3A.MZ50..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=28\n",
            "Trovato PGA nei metadati: 200.125946 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ50..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=28, componente N, PGA=200.125946\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ51 (15/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ51.zip\n",
            "Elaborazione file 3A.MZ51..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=29\n",
            "Trovato PGA nei metadati: 612.624939 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ51..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=29, componente E, PGA=612.624939\n",
            "Elaborazione file 3A.MZ51..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=30\n",
            "Trovato PGA nei metadati: 947.251343 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ51..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=30, componente N, PGA=947.251343\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ61 (16/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ61.zip\n",
            "Elaborazione file 3A.MZ61..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=31\n",
            "Trovato PGA nei metadati: 312.626099 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ61..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=31, componente E, PGA=312.626099\n",
            "Elaborazione file 3A.MZ61..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=32\n",
            "Trovato PGA nei metadati: 534.232422 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ61..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=32, componente N, PGA=534.232422\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione MZ63 (17/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_MZ63.zip\n",
            "Elaborazione file 3A.MZ63..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=33\n",
            "Trovato PGA nei metadati: 239.361557 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ63..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=33, componente E, PGA=239.361557\n",
            "Elaborazione file 3A.MZ63..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=34\n",
            "Trovato PGA nei metadati: 377.009308 cm/s²\n",
            "Rimossi 64 righe di metadati dal file 3A.MZ63..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=34, componente N, PGA=377.009308\n",
            "Elaborazione evento IT-2006-0272, stazione ANZI (18/34)\n",
            "File ZIP scaricato: ./data/raw/IT-2006-0272_ANZI.zip\n",
            "Elaborazione file BA.ANZI.00.HNE.D.IT-2006-0272.ACC.MP.ASC, AID=35\n",
            "Trovato PGA nei metadati: 0.248143 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.ANZI.00.HNE.D.IT-2006-0272.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=35, componente E, PGA=0.248143\n",
            "Elaborazione file BA.ANZI.00.HNN.D.IT-2006-0272.ACC.MP.ASC, AID=36\n",
            "Trovato PGA nei metadati: 0.453643 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.ANZI.00.HNN.D.IT-2006-0272.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=36, componente N, PGA=0.453643\n",
            "Elaborazione evento IT-2006-0272, stazione LTS (19/34)\n",
            "File ZIP scaricato: ./data/raw/IT-2006-0272_LTS.zip\n",
            "Elaborazione file BA.LTS.00.HNE.D.IT-2006-0272.ACC.MP.ASC, AID=37\n",
            "Trovato PGA nei metadati: 0.914947 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.LTS.00.HNE.D.IT-2006-0272.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=37, componente E, PGA=0.914947\n",
            "Elaborazione file BA.LTS.00.HNN.D.IT-2006-0272.ACC.MP.ASC, AID=38\n",
            "Trovato PGA nei metadati: 0.499435 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.LTS.00.HNN.D.IT-2006-0272.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=38, componente N, PGA=0.499435\n",
            "Elaborazione evento IT-2012-0011, stazione MIRE (20/34)\n",
            "File ZIP scaricato: ./data/raw/IT-2012-0011_MIRE.zip\n",
            "Elaborazione evento IT-2012-0011, stazione MIRH (21/34)\n",
            "File ZIP scaricato: ./data/raw/IT-2012-0011_MIRH.zip\n",
            "Elaborazione evento IT-2006-0272, stazione MRSN (22/34)\n",
            "File ZIP scaricato: ./data/raw/IT-2006-0272_MRSN.zip\n",
            "Elaborazione file BA.MRSN.00.HNE.D.IT-2006-0272.ACC.MP.ASC, AID=39\n",
            "Trovato PGA nei metadati: 0.39666 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.MRSN.00.HNE.D.IT-2006-0272.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=39, componente E, PGA=0.39666\n",
            "Elaborazione file BA.MRSN.00.HNN.D.IT-2006-0272.ACC.MP.ASC, AID=40\n",
            "Trovato PGA nei metadati: 0.526357 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.MRSN.00.HNN.D.IT-2006-0272.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=40, componente N, PGA=0.526357\n",
            "Elaborazione evento EMSC-20160824_0000006, stazione PZUN (23/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20160824_0000006_PZUN.zip\n",
            "Elaborazione file BA.PZUN..HNE.D.EMSC-20160824_0000006.ACC.MP.ASC, AID=41\n",
            "Trovato PGA nei metadati: 0.324755 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.PZUN..HNE.D.EMSC-20160824_0000006.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=41, componente E, PGA=0.324755\n",
            "Elaborazione file BA.PZUN..HNN.D.EMSC-20160824_0000006.ACC.MP.ASC, AID=42\n",
            "Trovato PGA nei metadati: 0.364869 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.PZUN..HNN.D.EMSC-20160824_0000006.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=42, componente N, PGA=0.364869\n",
            "Elaborazione evento EMSC-20161030_0000029, stazione PZUN (24/34)\n",
            "File ZIP scaricato: ./data/raw/EMSC-20161030_0000029_PZUN.zip\n",
            "Elaborazione file BA.PZUN..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=43\n",
            "Trovato PGA nei metadati: 0.589603 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.PZUN..HNE.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=43, componente E, PGA=0.589603\n",
            "Elaborazione file BA.PZUN..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC, AID=44\n",
            "Trovato PGA nei metadati: 0.640753 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.PZUN..HNN.D.EMSC-20161030_0000029.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=44, componente N, PGA=0.640753\n",
            "Elaborazione evento IT-2009-0009, stazione SANL (25/34)\n",
            "File ZIP scaricato: ./data/raw/IT-2009-0009_SANL.zip\n",
            "Elaborazione file BA.SANL.00.HNE.D.IT-2009-0009.ACC.MP.ASC, AID=45\n",
            "Trovato PGA nei metadati: 2.337397 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.SANL.00.HNE.D.IT-2009-0009.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=45, componente E, PGA=2.337397\n",
            "Elaborazione file BA.SANL.00.HNN.D.IT-2009-0009.ACC.MP.ASC, AID=46\n",
            "Trovato PGA nei metadati: 2.525746 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.SANL.00.HNN.D.IT-2009-0009.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=46, componente N, PGA=2.525746\n",
            "Elaborazione evento IT-2006-0272, stazione SCZM (26/34)\n",
            "File ZIP scaricato: ./data/raw/IT-2006-0272_SCZM.zip\n",
            "Elaborazione file BA.SCZM.00.HNE.D.IT-2006-0272.ACC.MP.ASC, AID=47\n",
            "Trovato PGA nei metadati: 5.379108 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.SCZM.00.HNE.D.IT-2006-0272.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=47, componente E, PGA=5.379108\n",
            "Elaborazione file BA.SCZM.00.HNN.D.IT-2006-0272.ACC.MP.ASC, AID=48\n",
            "Trovato PGA nei metadati: 6.184932 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.SCZM.00.HNN.D.IT-2006-0272.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=48, componente N, PGA=6.184932\n",
            "Elaborazione evento IT-2009-0009, stazione TTS (27/34)\n",
            "File ZIP scaricato: ./data/raw/IT-2009-0009_TTS.zip\n",
            "Elaborazione file BA.TTS.00.HNE.D.IT-2009-0009.ACC.MP.ASC, AID=49\n",
            "Trovato PGA nei metadati: 2.521722 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.TTS.00.HNE.D.IT-2009-0009.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=49, componente E, PGA=2.521722\n",
            "Elaborazione file BA.TTS.00.HNN.D.IT-2009-0009.ACC.MP.ASC, AID=50\n",
            "Trovato PGA nei metadati: 3.458751 cm/s²\n",
            "Rimossi 64 righe di metadati dal file BA.TTS.00.HNN.D.IT-2009-0009.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=50, componente N, PGA=3.458751\n",
            "Elaborazione evento IT-1976-0027, stazione BUI (28/34)\n",
            "File ZIP scaricato: ./data/raw/IT-1976-0027_BUI.zip\n",
            "Elaborazione file E.BUI.00.HNE.D.IT-1976-0027.ACC.MP.ASC, AID=51\n",
            "Trovato PGA nei metadati: 91.341034 cm/s²\n",
            "Rimossi 64 righe di metadati dal file E.BUI.00.HNE.D.IT-1976-0027.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=51, componente E, PGA=91.341034\n",
            "Elaborazione file E.BUI.00.HNN.D.IT-1976-0027.ACC.MP.ASC, AID=52\n",
            "Trovato PGA nei metadati: 108.496147 cm/s²\n",
            "Rimossi 64 righe di metadati dal file E.BUI.00.HNN.D.IT-1976-0027.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=52, componente N, PGA=108.496147\n",
            "Elaborazione evento IT-1976-0030, stazione BUI (29/34)\n",
            "File ZIP scaricato: ./data/raw/IT-1976-0030_BUI.zip\n",
            "Elaborazione file E.BUI.00.HNE.D.IT-1976-0030.ACC.MP.ASC, AID=53\n",
            "Trovato PGA nei metadati: 86.734421 cm/s²\n",
            "Rimossi 64 righe di metadati dal file E.BUI.00.HNE.D.IT-1976-0030.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=53, componente E, PGA=86.734421\n",
            "Elaborazione file E.BUI.00.HNN.D.IT-1976-0030.ACC.MP.ASC, AID=54\n",
            "Trovato PGA nei metadati: 79.497353 cm/s²\n",
            "Rimossi 64 righe di metadati dal file E.BUI.00.HNN.D.IT-1976-0030.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=54, componente N, PGA=79.497353\n",
            "Elaborazione evento IT-1976-0017, stazione FRC (30/34)\n",
            "File ZIP scaricato: ./data/raw/IT-1976-0017_FRC.zip\n",
            "Elaborazione file E.FRC.00.HNE.D.IT-1976-0017.ACC.MP.ASC, AID=55\n",
            "Trovato PGA nei metadati: 53.118771 cm/s²\n",
            "Rimossi 64 righe di metadati dal file E.FRC.00.HNE.D.IT-1976-0017.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=55, componente E, PGA=53.118771\n",
            "Elaborazione file E.FRC.00.HNN.D.IT-1976-0017.ACC.MP.ASC, AID=56\n",
            "Trovato PGA nei metadati: 44.279388 cm/s²\n",
            "Rimossi 64 righe di metadati dal file E.FRC.00.HNN.D.IT-1976-0017.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=56, componente N, PGA=44.279388\n",
            "Elaborazione evento IT-1976-0027, stazione FRC (31/34)\n",
            "File ZIP scaricato: ./data/raw/IT-1976-0027_FRC.zip\n",
            "Elaborazione file E.FRC.00.HNE.D.IT-1976-0027.ACC.MP.ASC, AID=57\n",
            "Trovato PGA nei metadati: 210.125275 cm/s²\n",
            "Rimossi 64 righe di metadati dal file E.FRC.00.HNE.D.IT-1976-0027.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=57, componente E, PGA=210.125275\n",
            "Elaborazione file E.FRC.00.HNN.D.IT-1976-0027.ACC.MP.ASC, AID=58\n",
            "Trovato PGA nei metadati: 258.808472 cm/s²\n",
            "Rimossi 64 righe di metadati dal file E.FRC.00.HNN.D.IT-1976-0027.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=58, componente N, PGA=258.808472\n",
            "Elaborazione evento IT-1976-0030, stazione FRC (32/34)\n",
            "File ZIP scaricato: ./data/raw/IT-1976-0030_FRC.zip\n",
            "Elaborazione file E.FRC.00.HNE.D.IT-1976-0030.ACC.MP.ASC, AID=59\n",
            "Trovato PGA nei metadati: 326.848022 cm/s²\n",
            "Rimossi 64 righe di metadati dal file E.FRC.00.HNE.D.IT-1976-0030.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=59, componente E, PGA=326.848022\n",
            "Elaborazione file E.FRC.00.HNN.D.IT-1976-0030.ACC.MP.ASC, AID=60\n",
            "Trovato PGA nei metadati: 341.508087 cm/s²\n",
            "Rimossi 64 righe di metadati dal file E.FRC.00.HNN.D.IT-1976-0030.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=60, componente N, PGA=341.508087\n",
            "Elaborazione evento IT-1976-0027, stazione SRC0 (33/34)\n",
            "File ZIP scaricato: ./data/raw/IT-1976-0027_SRC0.zip\n",
            "Elaborazione file E.SRC0.00.HNE.D.IT-1976-0027.ACC.MP.ASC, AID=61\n",
            "Trovato PGA nei metadati: 131.707764 cm/s²\n",
            "Rimossi 64 righe di metadati dal file E.SRC0.00.HNE.D.IT-1976-0027.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=61, componente E, PGA=131.707764\n",
            "Elaborazione file E.SRC0.00.HNN.D.IT-1976-0027.ACC.MP.ASC, AID=62\n",
            "Trovato PGA nei metadati: 58.345791 cm/s²\n",
            "Rimossi 64 righe di metadati dal file E.SRC0.00.HNN.D.IT-1976-0027.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=62, componente N, PGA=58.345791\n",
            "Elaborazione evento IT-1976-0030, stazione SRC0 (34/34)\n",
            "File ZIP scaricato: ./data/raw/IT-1976-0030_SRC0.zip\n",
            "Elaborazione file E.SRC0.00.HNE.D.IT-1976-0030.ACC.MP.ASC, AID=63\n",
            "Trovato PGA nei metadati: 244.807312 cm/s²\n",
            "Rimossi 64 righe di metadati dal file E.SRC0.00.HNE.D.IT-1976-0030.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=63, componente E, PGA=244.807312\n",
            "Elaborazione file E.SRC0.00.HNN.D.IT-1976-0030.ACC.MP.ASC, AID=64\n",
            "Trovato PGA nei metadati: 128.427597 cm/s²\n",
            "Rimossi 64 righe di metadati dal file E.SRC0.00.HNN.D.IT-1976-0030.ACC.MP.ASC\n",
            "Salvato accelerogramma AID=64, componente N, PGA=128.427597\n",
            "Elaborazione completata. Totale accelerogrammi elaborati: 64\n",
            "\n",
            "====== FASE 2: Decomposizione wavelet ======\n",
            "Trovati 64 file normalizzati da processare\n",
            "Elaborazione decomposizione wavelet per AID=37\n",
            "Elaborazione decomposizione wavelet per AID=25\n",
            "Elaborazione decomposizione wavelet per AID=7\n",
            "Elaborazione decomposizione wavelet per AID=36\n",
            "Elaborazione decomposizione wavelet per AID=59\n",
            "Elaborazione decomposizione wavelet per AID=1\n",
            "Elaborazione decomposizione wavelet per AID=21\n",
            "Elaborazione decomposizione wavelet per AID=5\n",
            "Elaborazione decomposizione wavelet per AID=26\n",
            "Elaborazione decomposizione wavelet per AID=52\n",
            "Elaborazione decomposizione wavelet per AID=57\n",
            "Elaborazione decomposizione wavelet per AID=49\n",
            "Elaborazione decomposizione wavelet per AID=55\n",
            "Elaborazione decomposizione wavelet per AID=62\n",
            "Elaborazione decomposizione wavelet per AID=12\n",
            "Elaborazione decomposizione wavelet per AID=50\n",
            "Elaborazione decomposizione wavelet per AID=23\n",
            "Elaborazione decomposizione wavelet per AID=3\n",
            "Elaborazione decomposizione wavelet per AID=58\n",
            "Elaborazione decomposizione wavelet per AID=2\n",
            "Elaborazione decomposizione wavelet per AID=4\n",
            "Elaborazione decomposizione wavelet per AID=15\n",
            "Elaborazione decomposizione wavelet per AID=40\n",
            "Elaborazione decomposizione wavelet per AID=42\n",
            "Elaborazione decomposizione wavelet per AID=18\n",
            "Elaborazione decomposizione wavelet per AID=13\n",
            "Elaborazione decomposizione wavelet per AID=17\n",
            "Elaborazione decomposizione wavelet per AID=61\n",
            "Elaborazione decomposizione wavelet per AID=33\n",
            "Elaborazione decomposizione wavelet per AID=6\n",
            "Elaborazione decomposizione wavelet per AID=34\n",
            "Elaborazione decomposizione wavelet per AID=53\n",
            "Elaborazione decomposizione wavelet per AID=22\n",
            "Elaborazione decomposizione wavelet per AID=35\n",
            "Elaborazione decomposizione wavelet per AID=16\n",
            "Elaborazione decomposizione wavelet per AID=64\n",
            "Elaborazione decomposizione wavelet per AID=60\n",
            "Elaborazione decomposizione wavelet per AID=44\n",
            "Elaborazione decomposizione wavelet per AID=9\n",
            "Elaborazione decomposizione wavelet per AID=10\n",
            "Elaborazione decomposizione wavelet per AID=39\n",
            "Elaborazione decomposizione wavelet per AID=47\n",
            "Elaborazione decomposizione wavelet per AID=8\n",
            "Elaborazione decomposizione wavelet per AID=54\n",
            "Elaborazione decomposizione wavelet per AID=43\n",
            "Elaborazione decomposizione wavelet per AID=51\n",
            "Elaborazione decomposizione wavelet per AID=19\n",
            "Elaborazione decomposizione wavelet per AID=30\n",
            "Elaborazione decomposizione wavelet per AID=63\n",
            "Elaborazione decomposizione wavelet per AID=24\n",
            "Elaborazione decomposizione wavelet per AID=38\n",
            "Elaborazione decomposizione wavelet per AID=32\n",
            "Elaborazione decomposizione wavelet per AID=56\n",
            "Elaborazione decomposizione wavelet per AID=41\n",
            "Elaborazione decomposizione wavelet per AID=28\n",
            "Elaborazione decomposizione wavelet per AID=31\n",
            "Elaborazione decomposizione wavelet per AID=20\n",
            "Elaborazione decomposizione wavelet per AID=11\n",
            "Elaborazione decomposizione wavelet per AID=48\n",
            "Elaborazione decomposizione wavelet per AID=46\n",
            "Elaborazione decomposizione wavelet per AID=45\n",
            "Elaborazione decomposizione wavelet per AID=27\n",
            "Elaborazione decomposizione wavelet per AID=29\n",
            "Elaborazione decomposizione wavelet per AID=14\n",
            "Elaborazione completata\n",
            "\n",
            "====== FASE 3: Addestramento del modello CGAN ======\n",
            "Utilizzo device: cpu\n",
            "Dataset wavelet inizializzato con 64 segnali sismici\n",
            "Inizio addestramento...\n",
            "[0/50][0/8] Loss_D: 1.3835 Loss_G: 1.3860 Time: 15.37s\n",
            "[1/50][0/8] Loss_D: 1.3864 Loss_G: 1.3855 Time: 102.48s\n",
            "[2/50][0/8] Loss_D: 1.3861 Loss_G: 1.3823 Time: 186.68s\n",
            "[3/50][0/8] Loss_D: 1.3853 Loss_G: 1.3865 Time: 275.58s\n",
            "[4/50][0/8] Loss_D: 1.3853 Loss_G: 1.3798 Time: 366.37s\n",
            "[5/50][0/8] Loss_D: 1.3820 Loss_G: 1.3862 Time: 452.07s\n",
            "[6/50][0/8] Loss_D: 1.3219 Loss_G: 1.3914 Time: 539.48s\n",
            "[7/50][0/8] Loss_D: 1.0102 Loss_G: 1.7504 Time: 630.87s\n",
            "[8/50][0/8] Loss_D: 0.9307 Loss_G: 2.3758 Time: 717.67s\n",
            "[9/50][0/8] Loss_D: 1.0177 Loss_G: 2.9479 Time: 810.67s\n",
            "[10/50][0/8] Loss_D: 1.0224 Loss_G: 2.0547 Time: 905.77s\n",
            "[11/50][0/8] Loss_D: 1.0364 Loss_G: 2.3438 Time: 999.77s\n",
            "[12/50][0/8] Loss_D: 1.1142 Loss_G: 2.3595 Time: 1091.78s\n",
            "[13/50][0/8] Loss_D: 1.0210 Loss_G: 2.0964 Time: 1182.18s\n",
            "[14/50][0/8] Loss_D: 1.0905 Loss_G: 2.1503 Time: 1278.78s\n",
            "[15/50][0/8] Loss_D: 0.9507 Loss_G: 2.6584 Time: 1371.28s\n",
            "[16/50][0/8] Loss_D: 0.9744 Loss_G: 2.5185 Time: 1467.48s\n",
            "[17/50][0/8] Loss_D: 1.0260 Loss_G: 2.7830 Time: 1558.98s\n",
            "[18/50][0/8] Loss_D: 0.9089 Loss_G: 2.7500 Time: 1651.58s\n",
            "[19/50][0/8] Loss_D: 0.9397 Loss_G: 3.3271 Time: 1743.78s\n",
            "[20/50][0/8] Loss_D: 0.8308 Loss_G: 2.7966 Time: 1839.08s\n",
            "[21/50][0/8] Loss_D: 1.0284 Loss_G: 3.6519 Time: 1929.48s\n",
            "[22/50][0/8] Loss_D: 0.9057 Loss_G: 3.6591 Time: 2022.08s\n",
            "[23/50][0/8] Loss_D: 0.8899 Loss_G: 5.1420 Time: 2114.18s\n",
            "[24/50][0/8] Loss_D: 0.7960 Loss_G: 2.9702 Time: 2204.58s\n",
            "[25/50][0/8] Loss_D: 0.8453 Loss_G: 2.6926 Time: 2298.07s\n",
            "[26/50][0/8] Loss_D: 0.8492 Loss_G: 3.2010 Time: 2392.87s\n",
            "[27/50][0/8] Loss_D: 0.7539 Loss_G: 4.5530 Time: 2482.87s\n",
            "[28/50][0/8] Loss_D: 0.7216 Loss_G: 4.0025 Time: 2574.28s\n",
            "[29/50][0/8] Loss_D: 0.6514 Loss_G: 3.6716 Time: 2666.77s\n",
            "[30/50][0/8] Loss_D: 0.6253 Loss_G: 3.5497 Time: 2759.87s\n",
            "[31/50][0/8] Loss_D: 0.5099 Loss_G: 3.7113 Time: 2850.38s\n",
            "[32/50][0/8] Loss_D: 0.4348 Loss_G: 4.5237 Time: 2939.98s\n",
            "[33/50][0/8] Loss_D: 0.4769 Loss_G: 4.1335 Time: 3027.18s\n",
            "[34/50][0/8] Loss_D: 0.3728 Loss_G: 4.8020 Time: 3116.58s\n",
            "[35/50][0/8] Loss_D: 0.4047 Loss_G: 5.4702 Time: 3209.18s\n",
            "[36/50][0/8] Loss_D: 0.3447 Loss_G: 4.3296 Time: 3297.67s\n",
            "[37/50][0/8] Loss_D: 0.3705 Loss_G: 4.7656 Time: 3386.08s\n",
            "[38/50][0/8] Loss_D: 0.3848 Loss_G: 4.4977 Time: 3478.98s\n",
            "[39/50][0/8] Loss_D: 0.5116 Loss_G: 6.2477 Time: 3568.78s\n",
            "[40/50][0/8] Loss_D: 0.3168 Loss_G: 5.1532 Time: 3656.47s\n",
            "[41/50][0/8] Loss_D: 0.3239 Loss_G: 6.2807 Time: 3750.18s\n",
            "[42/50][0/8] Loss_D: 0.3432 Loss_G: 6.5196 Time: 3843.67s\n",
            "[43/50][0/8] Loss_D: 0.3092 Loss_G: 5.1813 Time: 3936.38s\n",
            "[44/50][0/8] Loss_D: 0.3154 Loss_G: 6.1320 Time: 4032.48s\n",
            "[45/50][0/8] Loss_D: 0.3417 Loss_G: 6.8310 Time: 4125.28s\n",
            "[46/50][0/8] Loss_D: 0.2371 Loss_G: 4.9418 Time: 4221.57s\n",
            "[47/50][0/8] Loss_D: 0.3229 Loss_G: 5.8943 Time: 4317.58s\n",
            "[48/50][0/8] Loss_D: 0.3565 Loss_G: 6.7585 Time: 4414.17s\n",
            "[49/50][0/8] Loss_D: 0.3108 Loss_G: 6.7861 Time: 4508.28s\n",
            "Addestramento completato!\n",
            "\n",
            "====== FASE 4: Generazione e valutazione di accelerogrammi sintetici ======\n",
            "Generazione di accelerogrammi con le seguenti condizioni:\n",
            "Mw: 6.6, PGA: 0.1g\n",
            "Mw: 6.6, PGA: 0.2g\n",
            "Mw: 6.6, PGA: 0.3g\n",
            "Mw: 5.8, PGA: 0.1g\n",
            "Mw: 5.8, PGA: 0.2g\n",
            "Mw: 5.8, PGA: 0.3g\n",
            "Mw: 6.0, PGA: 0.1g\n",
            "Mw: 6.0, PGA: 0.2g\n",
            "Mw: 6.0, PGA: 0.3g\n",
            "Mw: 6.1, PGA: 0.1g\n",
            "Generazione accelerogrammi sintetici...\n",
            "Generazione accelerogramma 1/10 con Mw=6.6, PGA=0.10g\n",
            "Generazione accelerogramma 2/10 con Mw=6.6, PGA=0.20g\n",
            "Generazione accelerogramma 3/10 con Mw=6.6, PGA=0.30g\n",
            "Generazione accelerogramma 4/10 con Mw=5.8, PGA=0.10g\n",
            "Generazione accelerogramma 5/10 con Mw=5.8, PGA=0.20g\n",
            "Generazione accelerogramma 6/10 con Mw=5.8, PGA=0.30g\n",
            "Generazione accelerogramma 7/10 con Mw=6.0, PGA=0.10g\n",
            "Generazione accelerogramma 8/10 con Mw=6.0, PGA=0.20g\n",
            "Generazione accelerogramma 9/10 con Mw=6.0, PGA=0.30g\n",
            "Generazione accelerogramma 10/10 con Mw=6.1, PGA=0.10g\n",
            "Valutazione accelerogrammi sintetici...\n",
            "Metriche di valutazione:\n",
            "  mean: -0.017154159024357796\n",
            "  std: 0.560088574886322\n",
            "  min: -0.9999999403953552\n",
            "  max: 1.0\n",
            "  median: -0.002611452015116811\n",
            "\n",
            "Processo completato!\n"
          ]
        }
      ],
      "source": [
        "!pip install pywavelets\n",
        "\n",
        "# Generazione di Accelerogrammi Sintetici con CGAN e Decomposizione Wavelet\n",
        "\n",
        "# 1. Installazione delle dipendenze\n",
        "!pip install numpy pandas matplotlib torch pywavelets requests\n",
        "\n",
        "# 2. Importazioni\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pywt\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from google.colab import files\n",
        "\n",
        "# 3. Configurazione\n",
        "# @title Configurazione del progetto\n",
        "DATA_DIR = \"./data\"  # @param {type:\"string\"}\n",
        "OUTPUT_DIR = \"./output\"  # @param {type:\"string\"}\n",
        "USE_GPU = True  # @param {type:\"boolean\"}\n",
        "\n",
        "# Crea le directory di lavoro\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(DATA_DIR, \"raw\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(DATA_DIR, \"normalized\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(DATA_DIR, \"json\"), exist_ok=True)  # Corretto questa riga\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"models\"), exist_ok=True)\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, \"images\"), exist_ok=True)\n",
        "\n",
        "# Imposta device (CPU/GPU)\n",
        "device = torch.device(\"cuda\" if USE_GPU and torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Dispositivo utilizzato: {device}\")\n",
        "\n",
        "# Imposta device (CPU/GPU)\n",
        "device = torch.device(\"cuda\" if USE_GPU and torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Dispositivo utilizzato: {device}\")\n",
        "\n",
        "# 4. Funzioni di utilità per la gestione dei dati\n",
        "# @title Funzioni per l'acquisizione e preprocessing dei dati sismici\n",
        "\n",
        "def normalize_data(data):\n",
        "    \"\"\"\n",
        "    Normalizza i dati dell'accelerogramma\n",
        "\n",
        "    Args:\n",
        "        data: Array numpy con i dati dell'accelerogramma\n",
        "\n",
        "    Returns:\n",
        "        Array numpy con i dati normalizzati\n",
        "    \"\"\"\n",
        "    # Sottrazione della media\n",
        "    data_mean = np.mean(data)\n",
        "    data_centered = data - data_mean\n",
        "\n",
        "    # Divisione per deviazione standard\n",
        "    data_std = np.std(data_centered)\n",
        "    if data_std > 0:\n",
        "        data_normalized = data_centered / data_std\n",
        "    else:\n",
        "        data_normalized = data_centered  # Evita divisione per zero\n",
        "\n",
        "    return data_normalized\n",
        "\n",
        "def process_accelerogram(zip_ref, filename, aid, mw, epic_dist, component, output_dir):\n",
        "    \"\"\"\n",
        "    Processa un singolo file di accelerogramma\n",
        "\n",
        "    Args:\n",
        "        zip_ref: Riferimento all'archivio ZIP\n",
        "        filename: Nome del file nell'archivio\n",
        "        aid: ID dell'accelerogramma\n",
        "        mw: Magnitudo\n",
        "        epic_dist: Distanza epicentrale\n",
        "        component: Componente (E o N)\n",
        "        output_dir: Directory di output\n",
        "    \"\"\"\n",
        "    print(f\"Elaborazione file {filename}, AID={aid}\")\n",
        "\n",
        "    # Legge il contenuto del file\n",
        "    with zip_ref.open(filename) as f:\n",
        "        content = f.read().decode('utf-8', errors='replace').splitlines()\n",
        "\n",
        "    # Trova la fine dell'intestazione (dove iniziano i dati numerici)\n",
        "    data_start = 0\n",
        "    pga_value = None\n",
        "\n",
        "    # Cerca il PGA nei metadati\n",
        "    for i, line in enumerate(content):\n",
        "        if \"PGA_CM/S^2:\" in line:\n",
        "            parts = line.split(\":\")\n",
        "            if len(parts) > 1:\n",
        "                try:\n",
        "                    pga_value = abs(float(parts[1].strip()))\n",
        "                    print(f\"Trovato PGA nei metadati: {pga_value} cm/s²\")\n",
        "                except ValueError:\n",
        "                    pass\n",
        "\n",
        "        # Trova l'inizio dei dati numerici\n",
        "        try:\n",
        "            float(line.strip())\n",
        "            data_start = i\n",
        "            break\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    # Estrae solo i dati numerici (questo rimuove i metadati iniziali)\n",
        "    data_lines = content[data_start:]\n",
        "\n",
        "    # Stampa informazioni sulla rimozione dei metadati\n",
        "    print(f\"Rimossi {data_start} righe di metadati dal file {filename}\")\n",
        "\n",
        "    # Converte in array di float\n",
        "    try:\n",
        "        acceleration_data = np.array([float(line.strip()) for line in data_lines])\n",
        "    except ValueError as e:\n",
        "        print(f\"Errore nella conversione dei dati: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Se il PGA non è stato trovato nei metadati, calcolalo dai dati\n",
        "    if pga_value is None:\n",
        "        pga_value = np.max(np.abs(acceleration_data))\n",
        "        print(f\"PGA calcolato dai dati: {pga_value} cm/s²\")\n",
        "\n",
        "    # Estrai 10 secondi attorno al picco PGA (5 secondi prima e 5 secondi dopo)\n",
        "    fs = 200  # Frequenza di campionamento (Hz)\n",
        "    peak_idx = np.argmax(np.abs(acceleration_data))\n",
        "\n",
        "    # Calcola gli indici per la finestra di 10 secondi\n",
        "    start_idx = max(0, peak_idx - 5 * fs)\n",
        "    end_idx = min(len(acceleration_data), peak_idx + 5 * fs)\n",
        "\n",
        "    # Se la finestra è più corta di 10 secondi, adatta gli indici\n",
        "    if (end_idx - start_idx) < 10 * fs:\n",
        "        if start_idx == 0:  # Vicino all'inizio\n",
        "            end_idx = min(len(acceleration_data), start_idx + 10 * fs)\n",
        "        else:  # Vicino alla fine\n",
        "            start_idx = max(0, end_idx - 10 * fs)\n",
        "\n",
        "    # Estrai la finestra di 10 secondi\n",
        "    window_data = acceleration_data[start_idx:end_idx]\n",
        "\n",
        "    # Se la finestra non è esattamente 2000 campioni, ricampiona o taglia\n",
        "    if len(window_data) != 2000:\n",
        "        if len(window_data) > 2000:\n",
        "            # Taglia\n",
        "            excess = len(window_data) - 2000\n",
        "            window_data = window_data[excess//2:excess//2+2000]\n",
        "        else:\n",
        "            # Estendi con zeri\n",
        "            pad_width = 2000 - len(window_data)\n",
        "            window_data = np.pad(window_data, (pad_width//2, pad_width - pad_width//2), 'constant')\n",
        "\n",
        "    # Normalizza i dati: sottrazione della media e divisione per deviazione standard\n",
        "    normalized_data = normalize_data(window_data)\n",
        "\n",
        "    # Salva il file normalizzato\n",
        "    normalized_file = os.path.join(output_dir, \"normalized\", f\"{aid}.nrm\")\n",
        "    np.savetxt(normalized_file, normalized_data, fmt='%.8f')\n",
        "\n",
        "    # Crea e salva il file JSON con i metadati\n",
        "    metadata = {\n",
        "        \"AID\": aid,\n",
        "        \"MW\": float(mw),\n",
        "        \"epic_dist\": float(epic_dist),\n",
        "        \"PGA\": float(pga_value),\n",
        "        \"component\": component,\n",
        "        \"original_file\": filename\n",
        "    }\n",
        "\n",
        "    json_file = os.path.join(output_dir, \"json\", f\"{aid}.json\")\n",
        "    with open(json_file, 'w') as f:\n",
        "        json.dump(metadata, f, indent=4)\n",
        "\n",
        "    print(f\"Salvato accelerogramma AID={aid}, componente {component}, PGA={pga_value}\")\n",
        "\n",
        "def download_and_process_data(csv_path, output_dir=\"./data\"):\n",
        "    \"\"\"\n",
        "    Scarica e processa i dati sismici dal portale INGV (progetto ITACA)\n",
        "\n",
        "    Args:\n",
        "        csv_path: Percorso del file CSV con i dati degli eventi\n",
        "        output_dir: Directory di output per i file elaborati\n",
        "    \"\"\"\n",
        "    # Crea directory di output se non esistono\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, \"raw\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, \"normalized\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(output_dir, \"json\"), exist_ok=True)\n",
        "\n",
        "    # Carica il CSV con i dati degli eventi usando il separatore corretto (punto e virgola)\n",
        "    df = pd.read_csv(csv_path, sep=';')\n",
        "\n",
        "    # Stampa le colonne del CSV per il debug\n",
        "    print(\"Colonne disponibili nel CSV:\", df.columns.tolist())\n",
        "\n",
        "    print(f\"Trovati {len(df)} eventi nel file CSV\")\n",
        "\n",
        "    aid_counter = 1  # Contatore per l'ID degli accelerogrammi\n",
        "\n",
        "    # Itera sulle righe del CSV\n",
        "    for idx, row in df.iterrows():\n",
        "        try:\n",
        "            # Estrae i valori usando i nomi corretti delle colonne\n",
        "            event_id = row['esm_event_id']  # o 'ingv_event_id' se disponibile\n",
        "            station = row['station_code']\n",
        "            mw = row['mw']\n",
        "            epic_dist = row['epi_dist']  # o 'proximity' se disponibile\n",
        "\n",
        "            print(f\"Elaborazione evento {event_id}, stazione {station} ({idx+1}/{len(df)})\")\n",
        "\n",
        "            # Costruisce l'URL dell'API INGV\n",
        "            api_url = f\"https://itaca.mi.ingv.it/itaca40ws/eventdata/1/query?eventid={event_id}&station={station}&format=ascii\"\n",
        "\n",
        "            try:\n",
        "                # Scarica il file ZIP\n",
        "                response = requests.get(api_url)\n",
        "                response.raise_for_status()  # Solleva eccezione per errori HTTP\n",
        "\n",
        "                # Salva il file ZIP temporaneamente\n",
        "                temp_zip_path = os.path.join(output_dir, \"raw\", f\"{event_id}_{station}.zip\")\n",
        "                with open(temp_zip_path, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "\n",
        "                print(f\"File ZIP scaricato: {temp_zip_path}\")\n",
        "\n",
        "                # Estrai e processa i file dal ZIP\n",
        "                with zipfile.ZipFile(temp_zip_path, 'r') as zip_ref:\n",
        "                    # Lista tutti i file nell'archivio\n",
        "                    file_list = zip_ref.namelist()\n",
        "\n",
        "                    # Filtra i file per le componenti E e N\n",
        "                    e_files = [f for f in file_list if 'HGE' in f or 'HNE' in f]\n",
        "                    n_files = [f for f in file_list if 'HGN' in f or 'HNN' in f]\n",
        "\n",
        "                    # Processa i file componente E\n",
        "                    for e_file in e_files:\n",
        "                        process_accelerogram(zip_ref, e_file, aid_counter, mw, epic_dist, 'E', output_dir)\n",
        "                        aid_counter += 1\n",
        "\n",
        "                    # Processa i file componente N\n",
        "                    for n_file in n_files:\n",
        "                        process_accelerogram(zip_ref, n_file, aid_counter, mw, epic_dist, 'N', output_dir)\n",
        "                        aid_counter += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Errore durante l'elaborazione dell'evento {event_id}: {str(e)}\")\n",
        "                continue\n",
        "        except KeyError as e:\n",
        "            print(f\"Errore nell'accesso alla colonna: {str(e)} per la riga {idx+1}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"Elaborazione completata. Totale accelerogrammi elaborati: {aid_counter-1}\")\n",
        "\n",
        "# 5. Funzioni per la decomposizione wavelet\n",
        "# @title Funzioni per la decomposizione wavelet\n",
        "\n",
        "def wavelet_decomposition(signal, wavelet='db4', level=6):\n",
        "    \"\"\"\n",
        "    Scompone un segnale in componenti wavelet usando la trasformata wavelet discreta (DWT)\n",
        "\n",
        "    Args:\n",
        "        signal: Segnale da decomporre (array 1D)\n",
        "        wavelet: Famiglia wavelet da utilizzare (default: 'db4')\n",
        "        level: Livello di decomposizione (default: 6)\n",
        "\n",
        "    Returns:\n",
        "        Lista di componenti wavelet [A6, D1, D2, D3, D4, D5, D6]\n",
        "        dove A6 è l'approssimazione di livello 6 e Di sono i dettagli\n",
        "    \"\"\"\n",
        "    # Verifica che il segnale sia un array 1D\n",
        "    if isinstance(signal, list):\n",
        "        signal = np.array(signal)\n",
        "\n",
        "    if len(signal.shape) > 1:\n",
        "        signal = signal.flatten()\n",
        "\n",
        "    # Applica la decomposizione wavelet\n",
        "    coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
        "\n",
        "    # Estrai approssimazione (A6) e dettagli (D1-D6)\n",
        "    components = []\n",
        "\n",
        "    # Costruisci una lista di coefficienti per ogni componente\n",
        "    for i in range(len(coeffs)):\n",
        "        # Crea una copia dei coefficienti con tutti zero tranne l'i-esimo\n",
        "        coeff_i = []\n",
        "        for j in range(len(coeffs)):\n",
        "            if j == i:\n",
        "                coeff_i.append(coeffs[j])\n",
        "            else:\n",
        "                coeff_i.append(np.zeros_like(coeffs[j]))\n",
        "\n",
        "        # Ricostruisci il segnale con solo l'i-esimo coefficiente\n",
        "        rec = pywt.waverec(coeff_i, wavelet)\n",
        "\n",
        "        # Taglia alla lunghezza originale\n",
        "        if len(rec) > len(signal):\n",
        "            rec = rec[:len(signal)]\n",
        "        elif len(rec) < len(signal):\n",
        "            # Questo non dovrebbe accadere, ma gestiamo il caso per sicurezza\n",
        "            rec = np.pad(rec, (0, len(signal) - len(rec)), 'constant')\n",
        "\n",
        "        components.append(rec)\n",
        "\n",
        "    # Verifica che tutte le componenti abbiano la stessa lunghezza\n",
        "    target_length = 2000\n",
        "    for i in range(len(components)):\n",
        "        if len(components[i]) < target_length:\n",
        "            # Padding\n",
        "            components[i] = np.pad(components[i], (0, target_length - len(components[i])), 'constant')\n",
        "        elif len(components[i]) > target_length:\n",
        "            # Troncamento\n",
        "            components[i] = components[i][:target_length]\n",
        "\n",
        "    return components\n",
        "\n",
        "def get_component_energy(components):\n",
        "    \"\"\"\n",
        "    Calcola l'energia di ciascuna componente wavelet\n",
        "\n",
        "    Args:\n",
        "        components: Lista di componenti wavelet [A6, D1, D2, D3, D4, D5, D6]\n",
        "\n",
        "    Returns:\n",
        "        Array di energie normalizzate (somma a 1)\n",
        "    \"\"\"\n",
        "    # Calcola l'energia di ciascuna componente\n",
        "    energies = np.array([np.sum(comp**2) for comp in components])\n",
        "\n",
        "    # Normalizza le energie\n",
        "    total_energy = np.sum(energies)\n",
        "    if total_energy > 0:\n",
        "        normalized_energies = energies / total_energy\n",
        "    else:\n",
        "        normalized_energies = np.ones_like(energies) / len(energies)\n",
        "\n",
        "    return normalized_energies\n",
        "\n",
        "def plot_wavelet_decomposition(signal, components, save_path=None):\n",
        "    \"\"\"\n",
        "    Visualizza la decomposizione wavelet di un segnale\n",
        "\n",
        "    Args:\n",
        "        signal: Segnale originale\n",
        "        components: Lista di componenti wavelet [A6, D1, D2, D3, D4, D5, D6]\n",
        "        save_path: Percorso dove salvare l'immagine (opzionale)\n",
        "    \"\"\"\n",
        "    labels = ['A6', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6']\n",
        "    n_components = len(components)\n",
        "\n",
        "    # Crea la figura\n",
        "    fig, axs = plt.subplots(n_components + 1, 1, figsize=(12, 2.5 * (n_components + 1)))\n",
        "\n",
        "    # Plot del segnale originale\n",
        "    t = np.arange(len(signal)) / 200  # Assume 200 Hz sampling rate\n",
        "    axs[0].plot(t, signal)\n",
        "    axs[0].set_title('Segnale originale')\n",
        "    axs[0].set_ylabel('Ampiezza')\n",
        "\n",
        "    # Plot delle componenti\n",
        "    for i in range(n_components):\n",
        "        if i < len(labels):\n",
        "            label = labels[i]\n",
        "        else:\n",
        "            label = f'Componente {i}'\n",
        "\n",
        "        t_comp = np.arange(len(components[i])) / 200\n",
        "        axs[i+1].plot(t_comp, components[i])\n",
        "        axs[i+1].set_title(f'Componente {label}')\n",
        "        axs[i+1].set_ylabel('Ampiezza')\n",
        "\n",
        "    axs[-1].set_xlabel('Tempo (s)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Salva o mostra\n",
        "    if save_path:\n",
        "        plt.savefig(save_path)\n",
        "        plt.close()\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "def process_all_signals(data_dir, output_dir=None, wavelet='db4', level=6, plot=False):\n",
        "    \"\"\"\n",
        "    Processa tutti i segnali normalizzati e genera la decomposizione wavelet\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory contenente i file .nrm\n",
        "        output_dir: Directory di output per le componenti wavelet\n",
        "        wavelet: Famiglia wavelet da utilizzare\n",
        "        level: Livello di decomposizione\n",
        "        plot: Se True, genera e salva i plot\n",
        "    \"\"\"\n",
        "    # Se non specificato, usa sottodirectory della directory dati\n",
        "    if output_dir is None:\n",
        "        output_dir = os.path.join(data_dir, \"wavelet\")\n",
        "\n",
        "    # Crea directory di output se non esiste\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Directory per i plot\n",
        "    if plot:\n",
        "        plot_dir = os.path.join(output_dir, \"plots\")\n",
        "        os.makedirs(plot_dir, exist_ok=True)\n",
        "\n",
        "    # Trova tutti i file .nrm\n",
        "    normalized_dir = os.path.join(data_dir, \"normalized\")\n",
        "    nrm_files = [f for f in os.listdir(normalized_dir) if f.endswith('.nrm')]\n",
        "\n",
        "    print(f\"Trovati {len(nrm_files)} file normalizzati da processare\")\n",
        "\n",
        "    component_energies = {}\n",
        "\n",
        "    # Processa ogni file\n",
        "    for nrm_file in nrm_files:\n",
        "        aid = os.path.splitext(nrm_file)[0]\n",
        "        print(f\"Elaborazione decomposizione wavelet per AID={aid}\")\n",
        "\n",
        "        try:\n",
        "            # Carica il segnale normalizzato\n",
        "            signal_path = os.path.join(normalized_dir, nrm_file)\n",
        "            signal = np.loadtxt(signal_path)\n",
        "\n",
        "            # Applica la decomposizione wavelet\n",
        "            components = wavelet_decomposition(signal, wavelet, level)\n",
        "\n",
        "            # Calcola l'energia delle componenti\n",
        "            energies = get_component_energy(components)\n",
        "            component_energies[aid] = energies.tolist()  # Converti in lista per JSON\n",
        "\n",
        "            # Salva ogni componente in un file separato\n",
        "            comp_names = ['A6'] + [f'D{i}' for i in range(1, level + 1)]\n",
        "            for i, comp in enumerate(components):\n",
        "                if i < len(comp_names):\n",
        "                    comp_name = comp_names[i]\n",
        "                else:\n",
        "                    comp_name = f\"Comp{i}\"\n",
        "\n",
        "                comp_path = os.path.join(output_dir, f\"{aid}_{comp_name}.npy\")\n",
        "                np.save(comp_path, comp)\n",
        "\n",
        "            # Genera e salva il plot\n",
        "            if plot:\n",
        "                plot_path = os.path.join(plot_dir, f\"{aid}_wavelet.png\")\n",
        "                plot_wavelet_decomposition(signal, components, plot_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore durante l'elaborazione di AID={aid}: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "    # Salva le energie delle componenti\n",
        "    energies_path = os.path.join(output_dir, \"component_energies.json\")\n",
        "    with open(energies_path, 'w') as f:\n",
        "        json.dump(component_energies, f, indent=4)\n",
        "\n",
        "    print(\"Elaborazione completata\")\n",
        "\n",
        "# 6. Definizione dei modelli CGAN\n",
        "# @title Modelli CGAN (Generator e Discriminator)\n",
        "\n",
        "def weights_init(m):\n",
        "    \"\"\"\n",
        "    Inizializzazione dei pesi per migliorare la convergenza delle GAN\n",
        "\n",
        "    Args:\n",
        "        m: Modulo PyTorch\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "def textToVect(metadata):\n",
        "    \"\"\"\n",
        "    Converte i metadati in un vettore di condizionamento\n",
        "\n",
        "    Args:\n",
        "        metadata: Dizionario con MW (magnitudo) e PGA\n",
        "\n",
        "    Returns:\n",
        "        Vettore NumPy di dimensione 2 con [MW normalizzata, PGA normalizzata]\n",
        "    \"\"\"\n",
        "    # Estrazione dei valori\n",
        "    mw = float(metadata['MW'])\n",
        "    pga = float(metadata['PGA'])\n",
        "\n",
        "    # Normalizzazione empirica basata sui dati tipici\n",
        "    # Magnitudo: 4.0-7.0 -> [-1, 1]\n",
        "    norm_mw = 2 * (mw - 4.0) / 3.0 - 1\n",
        "\n",
        "    # PGA: fino a 1000 cm/s^2 -> [-1, 1]\n",
        "    norm_pga = 2 * (pga / 1000.0) - 1\n",
        "\n",
        "    # Clamp nei range validi\n",
        "    norm_mw = max(-1, min(1, norm_mw))\n",
        "    norm_pga = max(-1, min(1, norm_pga))\n",
        "\n",
        "    return np.array([norm_mw, norm_pga], dtype=np.float32)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    Generatore per la CGAN\n",
        "\n",
        "    Input:\n",
        "    - Vettore latente z (dimensione: latent_dim)\n",
        "    - Informazioni condizionali (dimensione: cond_dim)\n",
        "\n",
        "    Output:\n",
        "    - Accelerogramma sintetico (dimensione: 2000 campioni)\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim=100, cond_dim=2):\n",
        "        super(Generator, self).__init__()\n",
        "        self.latent_dim = latent_dim\n",
        "        self.cond_dim = cond_dim\n",
        "\n",
        "        # Dimensione totale dell'input (latent + condizioni)\n",
        "        self.input_dim = latent_dim + cond_dim\n",
        "\n",
        "        # Espansione iniziale\n",
        "        self.fc = nn.Linear(self.input_dim, 8000)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Reshape e convoluzioni\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            # Reshape a (batch_size, 250, 32)\n",
        "            # Prima convoluzione: (batch_size, 128, 250)\n",
        "            nn.Conv1d(32, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Espansione attraverso convoluzioni trasposte\n",
        "            # (batch_size, 64, 500)\n",
        "            nn.ConvTranspose1d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # (batch_size, 32, 1000)\n",
        "            nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # (batch_size, 16, 2000)\n",
        "            nn.ConvTranspose1d(32, 16, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Output finale: (batch_size, 1, 2000)\n",
        "            nn.Conv1d(16, 1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Tanh()  # Normalizza l'output tra -1 e 1\n",
        "        )\n",
        "\n",
        "    def forward(self, z, cond):\n",
        "        \"\"\"\n",
        "        Forward pass del generatore\n",
        "\n",
        "        Args:\n",
        "            z: Vettore latente (batch_size, latent_dim)\n",
        "            cond: Condizionamento (batch_size, cond_dim)\n",
        "\n",
        "        Returns:\n",
        "            Accelerogramma sintetico (batch_size, 2000)\n",
        "        \"\"\"\n",
        "        # Concatena rumore latente e condizioni\n",
        "        x = torch.cat([z, cond], dim=1)\n",
        "\n",
        "        # Espansione attraverso il layer fully connected\n",
        "        x = self.fc(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Reshape per convoluzione 1D: (batch_size, 32, 250)\n",
        "        x = x.view(-1, 32, 250)\n",
        "\n",
        "        # Applicazione dei blocchi convoluzionali\n",
        "        x = self.conv_blocks(x)\n",
        "\n",
        "        # Reshape finale: (batch_size, 2000)\n",
        "        x = x.view(-1, 2000)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Discriminatore per la CGAN\n",
        "\n",
        "    Input:\n",
        "    - Segnale (originale o generato) (dimensione: 2000 campioni)\n",
        "    - Informazioni condizionali (dimensione: cond_dim)\n",
        "\n",
        "    Output:\n",
        "    - Probabilità che il segnale sia reale (0-1)\n",
        "    \"\"\"\n",
        "    def __init__(self, cond_dim=2):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.cond_dim = cond_dim\n",
        "\n",
        "        # Blocchi convoluzionali per l'estrazione di feature\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            # Primo blocco\n",
        "            nn.Conv1d(1, 32, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            # Secondo blocco\n",
        "            nn.Conv1d(32, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            # Terzo blocco\n",
        "            nn.Conv1d(64, 128, kernel_size=7, stride=2, padding=3),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Dropout(0.3)\n",
        "        )\n",
        "\n",
        "        # Calcolo della dimensione dopo le convoluzioni\n",
        "        # Input: (1, 2000) -> Dopo 3 strati con stride=2: (128, 250)\n",
        "        conv_out_size = 128 * 250\n",
        "\n",
        "        # Fully connected finale con concatenazione delle condizioni\n",
        "        self.fc1 = nn.Linear(conv_out_size + cond_dim, 64)\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        \"\"\"\n",
        "        Forward pass del discriminatore\n",
        "\n",
        "        Args:\n",
        "            x: Segnale (batch_size, 2000)\n",
        "            cond: Condizionamento (batch_size, cond_dim)\n",
        "\n",
        "        Returns:\n",
        "            Probabilità (batch_size, 1)\n",
        "        \"\"\"\n",
        "        # Reshape per convoluzione 1D: (batch_size, 1, 2000)\n",
        "        x = x.view(-1, 1, 2000)\n",
        "\n",
        "        # Estrazione di feature tramite convoluzioni\n",
        "        x = self.conv_blocks(x)\n",
        "\n",
        "        # Flatten\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # Concatenazione con le condizioni\n",
        "        x = torch.cat([x, cond], dim=1)\n",
        "\n",
        "        # Fully connected finale\n",
        "        x = self.fc1(x)\n",
        "        x = self.leaky_relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# 7. Dataloader per CGAN\n",
        "# @title Dataloader per CGAN\n",
        "\n",
        "class SeismicWaveletDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset che carica le componenti wavelet dei segnali sismici\n",
        "    \"\"\"\n",
        "    def __init__(self, data_dir, transform=None, wavelet_level=6, decompose_on_fly=False):\n",
        "        \"\"\"\n",
        "        Inizializza il dataset\n",
        "\n",
        "        Args:\n",
        "            data_dir: Directory contenente i dati normalizzati e i metadati\n",
        "            transform: Trasformazioni opzionali da applicare ai dati\n",
        "            wavelet_level: Livello di decomposizione wavelet\n",
        "            decompose_on_fly: Se True, decompone il segnale al volo invece di caricare file precomputati\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "        self.wavelet_level = wavelet_level\n",
        "        self.decompose_on_fly = decompose_on_fly\n",
        "\n",
        "        # Directory contenenti i dati\n",
        "        self.normalized_dir = os.path.join(data_dir, \"normalized\")\n",
        "        self.json_dir = os.path.join(data_dir, \"json\")\n",
        "        self.wavelet_dir = os.path.join(data_dir, \"wavelet\")\n",
        "\n",
        "        # Lista di ID disponibili\n",
        "        self.aids = []\n",
        "        for file in os.listdir(self.normalized_dir):\n",
        "            if file.endswith('.nrm'):\n",
        "                aid = os.path.splitext(file)[0]\n",
        "                # Verifica che esista anche il json corrispondente\n",
        "                if os.path.exists(os.path.join(self.json_dir, f\"{aid}.json\")):\n",
        "                    self.aids.append(aid)\n",
        "\n",
        "        print(f\"Dataset wavelet inizializzato con {len(self.aids)} segnali sismici\")\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Restituisce il numero di segnali nel dataset\"\"\"\n",
        "        return len(self.aids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Carica un segnale sismico, le sue componenti wavelet e i suoi metadati\n",
        "\n",
        "        Args:\n",
        "            idx: Indice del segnale\n",
        "\n",
        "        Returns:\n",
        "            Tuple (componenti_wavelet, condizioni)\n",
        "            componenti_wavelet è una lista di tensori [A6, D1, D2, D3, D4, D5, D6]\n",
        "        \"\"\"\n",
        "        aid = self.aids[idx]\n",
        "\n",
        "        # Carica i metadati\n",
        "        json_path = os.path.join(self.json_dir, f\"{aid}.json\")\n",
        "        with open(json_path, 'r') as f:\n",
        "            metadata = json.load(f)\n",
        "\n",
        "        # Converti i metadati in vettore di condizionamento\n",
        "        cond_vector = textToVect(metadata)\n",
        "        cond_tensor = torch.FloatTensor(cond_vector)\n",
        "\n",
        "        # Se decomposizione al volo\n",
        "        if self.decompose_on_fly:\n",
        "            # Carica il segnale normalizzato\n",
        "            signal_path = os.path.join(self.normalized_dir, f\"{aid}.nrm\")\n",
        "            signal = np.loadtxt(signal_path)\n",
        "\n",
        "            # Decomponi il segnale\n",
        "            components = wavelet_decomposition(signal, wavelet='db4', level=self.wavelet_level)\n",
        "        else:\n",
        "            # Carica le componenti precomputate\n",
        "            components = []\n",
        "            # Approssimazione\n",
        "            a6_path = os.path.join(self.wavelet_dir, f\"{aid}_A{self.wavelet_level}.npy\")\n",
        "            if os.path.exists(a6_path):\n",
        "                a6 = np.load(a6_path)\n",
        "                components.append(a6)\n",
        "            else:\n",
        "                # Se il file non esiste, usa una decomposizione al volo\n",
        "                signal_path = os.path.join(self.normalized_dir, f\"{aid}.nrm\")\n",
        "                signal = np.loadtxt(signal_path)\n",
        "                components = wavelet_decomposition(signal, wavelet='db4', level=self.wavelet_level)\n",
        "\n",
        "                # Interrompi il ciclo\n",
        "                pass\n",
        "\n",
        "            # Dettagli\n",
        "            for i in range(1, self.wavelet_level + 1):\n",
        "                di_path = os.path.join(self.wavelet_dir, f\"{aid}_D{i}.npy\")\n",
        "                if os.path.exists(di_path):\n",
        "                    di = np.load(di_path)\n",
        "                    components.append(di)\n",
        "\n",
        "        # Assicurati che ci siano esattamente wavelet_level + 1 componenti\n",
        "        if len(components) != self.wavelet_level + 1:\n",
        "            raise ValueError(f\"Numero di componenti wavelet errato per AID {aid}: {len(components)}\")\n",
        "\n",
        "        # Verifica che tutte le componenti siano lunghe 2000 campioni\n",
        "        for i in range(len(components)):\n",
        "            if len(components[i]) != 2000:\n",
        "                # Taglia o estendi\n",
        "                if len(components[i]) > 2000:\n",
        "                    components[i] = components[i][:2000]\n",
        "                else:\n",
        "                    pad_width = 2000 - len(components[i])\n",
        "                    components[i] = np.pad(components[i], (0, pad_width), 'constant')\n",
        "\n",
        "        # Applica eventuali trasformazioni\n",
        "        if self.transform:\n",
        "            components = [self.transform(comp) for comp in components]\n",
        "\n",
        "        # Converte in tensori PyTorch\n",
        "        component_tensors = [torch.FloatTensor(comp) for comp in components]\n",
        "\n",
        "        return component_tensors, cond_tensor\n",
        "\n",
        "def get_dataloader(data_dir, batch_size=16, wavelet=True, wavelet_level=6, decompose_on_fly=False, shuffle=True, num_workers=2):\n",
        "    \"\"\"\n",
        "    Crea un DataLoader per i dati sismici\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory contenente i dati\n",
        "        batch_size: Dimensione del batch\n",
        "        wavelet: Se True, utilizza il dataset con decomposizione wavelet\n",
        "        wavelet_level: Livello di decomposizione wavelet\n",
        "        decompose_on_fly: Se True, decompone il segnale al volo invece di caricare file precomputati\n",
        "        shuffle: Se True, mescola i dati\n",
        "        num_workers: Numero di worker per il caricamento dati\n",
        "\n",
        "    Returns:\n",
        "        DataLoader\n",
        "    \"\"\"\n",
        "    dataset = SeismicWaveletDataset(\n",
        "        data_dir=data_dir,\n",
        "        wavelet_level=wavelet_level,\n",
        "        decompose_on_fly=decompose_on_fly\n",
        "    )\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        drop_last=True  # Necessario per batch normalizzazione\n",
        "    )\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "# 8. Addestramento della CGAN\n",
        "# @title Funzione di addestramento della CGAN\n",
        "\n",
        "def train_cgan(data_dir, output_dir=\"./output\", n_epochs=200, batch_size=16, latent_dim=100,\n",
        "               cond_dim=2, lr=0.0002, beta1=0.5, wavelet_level=6, save_interval=10):\n",
        "    \"\"\"\n",
        "    Addestra un modello CGAN con un singolo generatore e multi-discriminatori\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory contenente i dati\n",
        "        output_dir: Directory per i risultati\n",
        "        n_epochs: Numero di epoche di addestramento\n",
        "        batch_size: Dimensione del batch\n",
        "        latent_dim: Dimensione del vettore latente\n",
        "        cond_dim: Dimensione del vettore di condizionamento\n",
        "        lr: Learning rate\n",
        "        beta1: Parametro beta1 per Adam\n",
        "        wavelet_level: Livello di decomposizione wavelet\n",
        "        save_interval: Intervallo di salvataggio del modello\n",
        "    \"\"\"\n",
        "    # Crea directory di output\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    models_dir = os.path.join(output_dir, \"models\")\n",
        "    images_dir = os.path.join(output_dir, \"images\")\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    os.makedirs(images_dir, exist_ok=True)\n",
        "\n",
        "    # Imposta il device\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Utilizzo device: {device}\")\n",
        "\n",
        "    # Carica i dati\n",
        "    dataloader = get_dataloader(data_dir, batch_size=batch_size, wavelet=True,\n",
        "                               wavelet_level=wavelet_level, decompose_on_fly=True)\n",
        "\n",
        "    # Crea generatore e discriminatori\n",
        "    generator = Generator(latent_dim, cond_dim).to(device)\n",
        "    discriminators = [Discriminator(cond_dim).to(device) for _ in range(wavelet_level + 1)]\n",
        "\n",
        "    # Inizializza i pesi\n",
        "    generator.apply(weights_init)\n",
        "    for disc in discriminators:\n",
        "        disc.apply(weights_init)\n",
        "\n",
        "    # Setup ottimizzatori\n",
        "    optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "    optimizer_Ds = [\n",
        "        optim.Adam(disc.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "        for disc in discriminators\n",
        "    ]\n",
        "\n",
        "    # Criterio di perdita\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    # Pesi uniformi iniziali per i discriminatori\n",
        "    alphas = np.ones(wavelet_level + 1) / (wavelet_level + 1)\n",
        "\n",
        "    # Log per tenere traccia dei progressi\n",
        "    losses_G = []\n",
        "    losses_D = []\n",
        "\n",
        "    print(\"Inizio addestramento...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for i, (components, conditions) in enumerate(dataloader):\n",
        "            batch_size_actual = conditions.size(0)\n",
        "\n",
        "            # Verifica che ci siano dati sufficienti per procedere\n",
        "            if batch_size_actual == 0:\n",
        "                print(\"Batch vuoto, skippo\")\n",
        "                continue\n",
        "\n",
        "            # Preparazione delle etichette\n",
        "            real_label = torch.ones(batch_size_actual, 1, device=device)\n",
        "            fake_label = torch.zeros(batch_size_actual, 1, device=device)\n",
        "\n",
        "            # Passa le condizioni al device corretto\n",
        "            conditions = conditions.to(device)\n",
        "\n",
        "            # =============================================================\n",
        "            # (1) Aggiorna i discriminatori: max log(D(x)) + log(1 - D(G(z)))\n",
        "            # =============================================================\n",
        "            # Addestra ogni discriminatore separatamente\n",
        "            d_losses = []\n",
        "            for j, discriminator in enumerate(discriminators):\n",
        "                optimizer_Ds[j].zero_grad()\n",
        "\n",
        "                # Verifica che ci siano abbastanza componenti\n",
        "                if j >= len(components):\n",
        "                    print(f\"Avviso: Mancano componenti per il discriminatore {j}\")\n",
        "                    d_losses.append(0.0)  # Aggiungi un valore fittizio alla perdita\n",
        "                    continue\n",
        "\n",
        "                # Carica la componente wavelet j-esima\n",
        "                real_comp = components[j].to(device)\n",
        "\n",
        "                # Verifica che le dimensioni siano corrette\n",
        "                if real_comp.dim() == 1:\n",
        "                    real_comp = real_comp.unsqueeze(0)  # Aggiungi dimensione batch se mancante\n",
        "\n",
        "                # Calcola output del discriminatore con dati reali\n",
        "                output_real = discriminator(real_comp, conditions)\n",
        "                d_loss_real = criterion(output_real, real_label)\n",
        "\n",
        "                # Genera dati falsi\n",
        "                z = torch.randn(batch_size_actual, latent_dim, device=device)\n",
        "                fake_signal = generator(z, conditions)\n",
        "\n",
        "                # Decomponi il segnale falso in componenti wavelet\n",
        "                # Usa il CPU per la decomposizione wavelet (più compatibile)\n",
        "                # Importante: usa detach() per staccare dal grafo computazionale\n",
        "                fake_signal_np = fake_signal.detach().cpu().numpy()  # Aggiunto detach()\n",
        "\n",
        "                # Decomponi ogni segnale individualmente nel batch\n",
        "                fake_comps_list = []\n",
        "                for k in range(fake_signal_np.shape[0]):\n",
        "                    comps = wavelet_decomposition(fake_signal_np[k], level=wavelet_level)\n",
        "                    fake_comps_list.append(comps[j])  # Prendi solo la componente j-esima\n",
        "\n",
        "                # Converti la lista in un tensore batch\n",
        "                fake_comp_np = np.stack(fake_comps_list)\n",
        "                fake_comp = torch.tensor(fake_comp_np, device=device, dtype=torch.float32)\n",
        "\n",
        "                # Calcola output del discriminatore con dati falsi\n",
        "                output_fake = discriminator(fake_comp, conditions)\n",
        "                d_loss_fake = criterion(output_fake, fake_label)\n",
        "\n",
        "                # Perdita totale per il discriminatore j-esimo\n",
        "                d_loss = d_loss_real + d_loss_fake\n",
        "                d_loss.backward()\n",
        "                optimizer_Ds[j].step()\n",
        "\n",
        "                d_losses.append(d_loss.item())\n",
        "\n",
        "            # =============================================================\n",
        "            # (2) Aggiorna il generatore: max log(D(G(z)))\n",
        "            # =============================================================\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            # Genera nuovi dati falsi\n",
        "            z = torch.randn(batch_size_actual, latent_dim, device=device)\n",
        "            fake_signal = generator(z, conditions)\n",
        "\n",
        "            # Decomponi nuovamente il segnale falso\n",
        "            # Usa il CPU per la decomposizione wavelet\n",
        "            # Qui NON usare detach() perché abbiamo bisogno dei gradienti\n",
        "            fake_signal_np = fake_signal.cpu().detach().numpy()  # Aggiunto detach()\n",
        "\n",
        "            # Perdita del generatore complessiva\n",
        "            g_loss = 0\n",
        "\n",
        "            # Calcola perdita per ogni discriminatore\n",
        "            for j, discriminator in enumerate(discriminators):\n",
        "                if j >= wavelet_level + 1:\n",
        "                    continue  # Skip se indice fuori range\n",
        "\n",
        "                # Decomponi ogni segnale nel batch\n",
        "                fake_comps_list = []\n",
        "                for k in range(fake_signal_np.shape[0]):\n",
        "                    comps = wavelet_decomposition(fake_signal_np[k], level=wavelet_level)\n",
        "                    if j < len(comps):\n",
        "                        fake_comps_list.append(comps[j])\n",
        "                    else:\n",
        "                        # Se per qualche motivo la decomposizione non ha abbastanza componenti\n",
        "                        print(f\"Avviso: Decomposizione incompleta per indice {j}\")\n",
        "                        # Crea una componente vuota\n",
        "                        fake_comps_list.append(np.zeros(2000))\n",
        "\n",
        "                # Converti la lista in un tensore batch\n",
        "                fake_comp_np = np.stack(fake_comps_list)\n",
        "                fake_comp = torch.tensor(fake_comp_np, device=device, dtype=torch.float32)\n",
        "\n",
        "                # Il generatore vuole che il discriminatore classifichi i segnali come reali\n",
        "                output = discriminator(fake_comp, conditions)\n",
        "                component_loss = criterion(output, real_label)\n",
        "                g_loss += alphas[j] * component_loss\n",
        "\n",
        "            # Aggiungi anche il loss diretto (senza decomposizione)\n",
        "            direct_output = discriminators[0](fake_signal, conditions)\n",
        "            direct_loss = criterion(direct_output, real_label)\n",
        "            g_loss += direct_loss\n",
        "\n",
        "            # Backpropagation\n",
        "            g_loss.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "            # =============================================================\n",
        "            # Calcola nuovi pesi alpha basati sull'energia delle componenti\n",
        "            # =============================================================\n",
        "            if i % 10 == 0:  # Aggiorna i pesi ogni 10 batch\n",
        "                # Prendi un batch di segnali reali e calcola le energie\n",
        "                try:\n",
        "                    # Seleziona un campione casuale di componenti\n",
        "                    sample_idx = np.random.randint(0, batch_size_actual, min(4, batch_size_actual))\n",
        "                    energies = []\n",
        "\n",
        "                    for idx in sample_idx:\n",
        "                        # Prendi tutte le componenti per questo campione\n",
        "                        real_comps = [comp[idx].cpu().numpy() for comp in components]\n",
        "                        batch_energies = get_component_energy(real_comps)\n",
        "                        energies.append(batch_energies)\n",
        "\n",
        "                    # Media le energie su tutti i batch campionati\n",
        "                    if energies:\n",
        "                        mean_energies = np.mean(energies, axis=0)\n",
        "                        # Assicurati che ci siano esattamente wavelet_level + 1 pesi\n",
        "                        if len(mean_energies) == wavelet_level + 1:\n",
        "                            alphas = mean_energies\n",
        "                except Exception as e:\n",
        "                    print(f\"Errore nel calcolo delle energie: {str(e)}\")\n",
        "\n",
        "            # =============================================================\n",
        "            # Log\n",
        "            # =============================================================\n",
        "            if i % 10 == 0:\n",
        "                elapsed = time.time() - start_time\n",
        "                d_loss_mean = np.mean([l for l in d_losses if l > 0])  # Media solo delle perdite valide\n",
        "                print(f\"[{epoch}/{n_epochs}][{i}/{len(dataloader)}] \"\n",
        "                      f\"Loss_D: {d_loss_mean:.4f} Loss_G: {g_loss.item():.4f} \"\n",
        "                      f\"Time: {elapsed:.2f}s\")\n",
        "\n",
        "                # Salva le perdite per il plot\n",
        "                losses_G.append(g_loss.item())\n",
        "                losses_D.append(d_loss_mean)\n",
        "\n",
        "                # Genera e salva un esempio\n",
        "                with torch.no_grad():\n",
        "                    # Genera un singolo esempio\n",
        "                    z = torch.randn(1, latent_dim, device=device)\n",
        "                    # Usa la prima condizione del batch come esempio\n",
        "                    fixed_cond = conditions[0].unsqueeze(0) if batch_size_actual > 0 else torch.tensor([[0.33, 0.0]], device=device)\n",
        "\n",
        "                    fake = generator(z, fixed_cond).cpu().numpy()[0]\n",
        "\n",
        "                    # Plot\n",
        "                    plt.figure(figsize=(10, 4))\n",
        "                    plt.plot(fake)\n",
        "                    plt.title(f\"Accelerogramma sintetico - Epoca {epoch}\")\n",
        "                    plt.xlabel(\"Campioni\")\n",
        "                    plt.ylabel(\"Accelerazione (normalizzata)\")\n",
        "                    plt.savefig(os.path.join(images_dir, f\"epoch_{epoch:03d}_batch_{i:04d}.png\"))\n",
        "                    plt.close()\n",
        "\n",
        "        # Salva il modello periodicamente\n",
        "        if (epoch + 1) % save_interval == 0 or epoch == n_epochs - 1:\n",
        "            try:\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'generator_state_dict': generator.state_dict(),\n",
        "                    'discriminator_state_dicts': [disc.state_dict() for disc in discriminators],\n",
        "                    'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "                    'optimizer_Ds_state_dict': [opt.state_dict() for opt in optimizer_Ds],\n",
        "                    'losses_G': losses_G,\n",
        "                    'losses_D': losses_D,\n",
        "                    'alphas': alphas.tolist() if isinstance(alphas, np.ndarray) else alphas\n",
        "                }, os.path.join(models_dir, f\"model_epoch_{epoch:03d}.pt\"))\n",
        "\n",
        "                # Salva anche l'ultimo modello\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'generator_state_dict': generator.state_dict(),\n",
        "                    'discriminator_state_dicts': [disc.state_dict() for disc in discriminators],\n",
        "                    'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "                    'optimizer_Ds_state_dict': [opt.state_dict() for opt in optimizer_Ds],\n",
        "                    'losses_G': losses_G,\n",
        "                    'losses_D': losses_D,\n",
        "                    'alphas': alphas.tolist() if isinstance(alphas, np.ndarray) else alphas\n",
        "                }, os.path.join(models_dir, \"latest_model.pt\"))\n",
        "            except Exception as e:\n",
        "                print(f\"Errore nel salvataggio del modello: {str(e)}\")\n",
        "\n",
        "    # Plot finale delle perdite\n",
        "    try:\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(losses_G, label='Generator')\n",
        "        plt.plot(losses_D, label='Discriminator')\n",
        "        plt.xlabel('Iterations (x50)')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.title('Training Losses')\n",
        "        plt.savefig(os.path.join(output_dir, \"training_losses.png\"))\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Errore nella creazione del plot delle perdite: {str(e)}\")\n",
        "\n",
        "    print(\"Addestramento completato!\")\n",
        "    return generator, discriminators\n",
        "\n",
        "# 9. Generazione e valutazione di accelerogrammi sintetici\n",
        "# @title Funzioni per generazione e valutazione\n",
        "\n",
        "def load_model(model_path, latent_dim=100, cond_dim=2):\n",
        "    \"\"\"\n",
        "    Carica un modello pre-addestrato\n",
        "\n",
        "    Args:\n",
        "        model_path: Percorso del file del modello\n",
        "        latent_dim: Dimensione del vettore latente\n",
        "        cond_dim: Dimensione del vettore di condizionamento\n",
        "\n",
        "    Returns:\n",
        "        Generatore caricato\n",
        "    \"\"\"\n",
        "    # Crea il generatore\n",
        "    generator = Generator(latent_dim, cond_dim)\n",
        "\n",
        "    # Carica i pesi del modello con weights_only=False per compatibilità\n",
        "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'), weights_only=False)\n",
        "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "\n",
        "    # Imposta il generatore in modalità di valutazione (non addestramento)\n",
        "    generator.eval()\n",
        "\n",
        "    return generator\n",
        "\n",
        "def generate_accelerograms(generator, n_samples=10, conditions=None, output_dir=None, latent_dim=100):\n",
        "    \"\"\"\n",
        "    Genera accelerogrammi sintetici utilizzando un generatore addestrato\n",
        "\n",
        "    Args:\n",
        "        generator: Generatore addestrato\n",
        "        n_samples: Numero di campioni da generare\n",
        "        conditions: Lista di coppie (mw, pga) da utilizzare per il condizionamento\n",
        "                   Se None, utilizza valori casuali\n",
        "        output_dir: Directory dove salvare i risultati\n",
        "        latent_dim: Dimensione del vettore latente\n",
        "\n",
        "    Returns:\n",
        "        Lista di accelerogrammi sintetici\n",
        "    \"\"\"\n",
        "    if output_dir:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    device = next(generator.parameters()).device\n",
        "\n",
        "    # Se non vengono fornite condizioni, genera valori casuali\n",
        "    if conditions is None:\n",
        "        # Genera magnitudo tra 4.0 e 7.0\n",
        "        mw_values = np.random.uniform(4.0, 7.0, n_samples)\n",
        "\n",
        "        # Genera PGA tra 0.05 e 0.5 g\n",
        "        pga_values = np.random.uniform(0.05, 0.5, n_samples)\n",
        "\n",
        "        conditions = list(zip(mw_values, pga_values))\n",
        "\n",
        "    # Genera gli accelerogrammi\n",
        "    synthetic_signals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (mw, pga) in enumerate(conditions):\n",
        "            print(f\"Generazione accelerogramma {i+1}/{len(conditions)} con Mw={mw:.1f}, PGA={pga:.2f}g\")\n",
        "\n",
        "            # Crea dizionario per textToVect\n",
        "            metadata = {'MW': mw, 'PGA': pga}\n",
        "\n",
        "            # Converti in vettore di condizionamento\n",
        "            cond_vect = textToVect(metadata)\n",
        "            cond_tensor = torch.FloatTensor(cond_vect).unsqueeze(0).to(device)\n",
        "\n",
        "            # Genera rumore casuale\n",
        "            z = torch.randn(1, latent_dim, device=device)\n",
        "\n",
        "            # Genera l'accelerogramma\n",
        "            fake_signal = generator(z, cond_tensor).cpu().numpy()[0]\n",
        "            synthetic_signals.append(fake_signal)\n",
        "\n",
        "            # Salva l'accelerogramma\n",
        "            if output_dir:\n",
        "                # Salva i dati del segnale\n",
        "                signal_path = os.path.join(output_dir, f\"synthetic_{i+1:03d}.npy\")\n",
        "                np.save(signal_path, fake_signal)\n",
        "\n",
        "                # Salva i metadati\n",
        "                metadata_path = os.path.join(output_dir, f\"synthetic_{i+1:03d}.json\")\n",
        "                metadata_obj = {\n",
        "                    'id': f\"synthetic_{i+1:03d}\",\n",
        "                    'mw': float(mw),\n",
        "                    'pga': float(pga)\n",
        "                }\n",
        "                with open(metadata_path, 'w') as f:\n",
        "                    json.dump(metadata_obj, f, indent=4)\n",
        "\n",
        "                # Crea e salva il plot\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                t = np.arange(len(fake_signal)) / 200  # Assume 200 Hz sampling rate\n",
        "                plt.plot(t, fake_signal)\n",
        "                plt.title(f\"Accelerogramma sintetico (Mw={mw:.1f}, PGA={pga:.2f}g)\")\n",
        "                plt.xlabel(\"Tempo (s)\")\n",
        "                plt.ylabel(\"Accelerazione (normalizzata)\")\n",
        "                plt.grid(True)\n",
        "                plt.savefig(os.path.join(output_dir, f\"synthetic_{i+1:03d}.png\"))\n",
        "                plt.close()\n",
        "\n",
        "                # Opzionalmente, genera anche la decomposizione wavelet\n",
        "                components = wavelet_decomposition(fake_signal, level=6)\n",
        "\n",
        "                # Crea plot delle componenti\n",
        "                plt.figure(figsize=(12, 15))\n",
        "\n",
        "                # Plot del segnale originale\n",
        "                plt.subplot(8, 1, 1)\n",
        "                plt.plot(t, fake_signal)\n",
        "                plt.title(f\"Accelerogramma sintetico (Mw={mw:.1f}, PGA={pga:.2f}g)\")\n",
        "                plt.ylabel(\"Ampiezza\")\n",
        "                plt.grid(True)\n",
        "\n",
        "                # Plot delle componenti\n",
        "                labels = ['A6', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6']\n",
        "                for j, comp in enumerate(components):\n",
        "                    if j < len(labels):\n",
        "                        plt.subplot(8, 1, j+2)\n",
        "                        plt.plot(t, comp)\n",
        "                        plt.title(f\"Componente {labels[j]}\")\n",
        "                        plt.ylabel(\"Ampiezza\")\n",
        "                        plt.grid(True)\n",
        "\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join(output_dir, f\"synthetic_{i+1:03d}_wavelet.png\"))\n",
        "                plt.close()\n",
        "\n",
        "    return synthetic_signals\n",
        "\n",
        "def evaluate_accelerograms(synthetic_signals, output_dir=None):\n",
        "    \"\"\"\n",
        "    Valuta gli accelerogrammi sintetici\n",
        "\n",
        "    Args:\n",
        "        synthetic_signals: Lista di accelerogrammi sintetici\n",
        "        output_dir: Directory dove salvare i risultati\n",
        "\n",
        "    Returns:\n",
        "        Dizionario con le metriche di valutazione\n",
        "    \"\"\"\n",
        "    if output_dir:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    # Analisi statistica dei segnali sintetici\n",
        "    amplitudes = np.concatenate([signal for signal in synthetic_signals])\n",
        "\n",
        "    metrics['mean'] = float(np.mean(amplitudes))\n",
        "    metrics['std'] = float(np.std(amplitudes))\n",
        "    metrics['min'] = float(np.min(amplitudes))\n",
        "    metrics['max'] = float(np.max(amplitudes))\n",
        "    metrics['median'] = float(np.median(amplitudes))\n",
        "\n",
        "    # Crea un istogramma delle ampiezze\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(amplitudes, bins=50, alpha=0.7, color='blue')\n",
        "    plt.title('Distribuzione delle ampiezze degli accelerogrammi sintetici')\n",
        "    plt.xlabel('Ampiezza')\n",
        "    plt.ylabel('Frequenza')\n",
        "    plt.grid(True)\n",
        "\n",
        "    if output_dir:\n",
        "        plt.savefig(os.path.join(output_dir, 'amplitude_distribution.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Analisi spettrale\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    # Calcolo dello spettro di potenza medio\n",
        "    psd_all = []\n",
        "\n",
        "    for i, signal in enumerate(synthetic_signals):\n",
        "        # Calcola la trasformata di Fourier\n",
        "        fft_result = np.fft.fft(signal)\n",
        "\n",
        "        # Calcola le frequenze corrispondenti (assumendo 200 Hz di campionamento)\n",
        "        n = len(signal)\n",
        "        fs = 200  # Hz\n",
        "        freqs = np.fft.fftfreq(n, 1/fs)\n",
        "\n",
        "        # Calcola la densità spettrale di potenza (PSD)\n",
        "        psd = np.abs(fft_result)**2 / n\n",
        "\n",
        "        # Considera solo le frequenze positive\n",
        "        positive_freqs = freqs[1:n//2]\n",
        "        positive_psd = psd[1:n//2]\n",
        "\n",
        "        # Aggiungi alla lista\n",
        "        psd_all.append(positive_psd)\n",
        "\n",
        "        # Plot individuale (solo alcuni per leggibilità)\n",
        "        if i < 5:  # Mostra solo i primi 5 spettri\n",
        "            plt.semilogy(positive_freqs, positive_psd, alpha=0.3,\n",
        "                      label=f'Segnale sintetico {i+1}')\n",
        "\n",
        "    # Calcola e plotta lo spettro medio\n",
        "    if psd_all:\n",
        "        mean_psd = np.mean(psd_all, axis=0)\n",
        "        plt.semilogy(positive_freqs, mean_psd, 'k-', linewidth=2,\n",
        "                  label='Media dei segnali sintetici')\n",
        "\n",
        "    plt.title('Analisi spettrale degli accelerogrammi sintetici')\n",
        "    plt.xlabel('Frequenza (Hz)')\n",
        "    plt.ylabel('Densità spettrale di potenza')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    if output_dir:\n",
        "        plt.savefig(os.path.join(output_dir, 'spectral_analysis.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # Salva le metriche\n",
        "    if output_dir:\n",
        "        with open(os.path.join(output_dir, 'evaluation_metrics.json'), 'w') as f:\n",
        "            json.dump(metrics, f, indent=4)\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# 10. Interfaccia principale\n",
        "# @title Caricamento del file CSV con dati reali\n",
        "print(\"Per favore, carica il file CSV contenente i dati degli eventi sismici reali.\")\n",
        "print(\"Il file deve contenere le colonne: esm_event_id, station_code, mw, epi_dist\")\n",
        "print(\"Formato CSV con separatore punto e virgola (;)\")\n",
        "\n",
        "# Carica il file CSV con i dati reali\n",
        "csv_upload = files.upload()\n",
        "\n",
        "if len(csv_upload) == 0:\n",
        "    raise ValueError(\"Nessun file caricato. È necessario caricare un file CSV per procedere.\")\n",
        "\n",
        "# Ottieni il nome del primo file caricato\n",
        "csv_filename = list(csv_upload.keys())[0]\n",
        "\n",
        "# Verifica che sia un file CSV\n",
        "if not csv_filename.endswith('.csv'):\n",
        "    raise ValueError(f\"Il file caricato ({csv_filename}) non è un file CSV.\")\n",
        "\n",
        "# Copia il file nella directory corrente\n",
        "with open(csv_filename, 'wb') as f:\n",
        "    f.write(csv_upload[csv_filename])\n",
        "\n",
        "print(f\"File CSV caricato con successo: {csv_filename}\")\n",
        "\n",
        "# Verifica il contenuto del file\n",
        "try:\n",
        "    df = pd.read_csv(csv_filename, sep=';')\n",
        "    required_columns = ['esm_event_id', 'station_code', 'mw', 'epi_dist']\n",
        "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
        "\n",
        "    if missing_columns:\n",
        "        print(f\"ATTENZIONE: Il file manca delle seguenti colonne: {', '.join(missing_columns)}\")\n",
        "    else:\n",
        "        print(f\"Il file contiene {len(df)} eventi sismici.\")\n",
        "        print(\"Prime righe del file:\")\n",
        "        print(df.head())\n",
        "except Exception as e:\n",
        "    print(f\"Errore nella lettura del file CSV: {str(e)}\")\n",
        "\n",
        "# 11. Flusso di lavoro principale\n",
        "# @title Esecuzione del flusso di lavoro\n",
        "execute_download = True  # @param {type:\"boolean\"}\n",
        "execute_wavelet = True  # @param {type:\"boolean\"}\n",
        "execute_training = True  # @param {type:\"boolean\"}\n",
        "execute_generation = True  # @param {type:\"boolean\"}\n",
        "n_epochs = 50  # @param {type:\"slider\", min:10, max:500, step:10}\n",
        "batch_size = 8  # @param {type:\"slider\", min:1, max:32, step:1}\n",
        "\n",
        "# Step 1: Download e preprocessamento dei dati\n",
        "if execute_download:\n",
        "    print(\"\\n====== FASE 1: Download e preprocessamento dei dati ======\")\n",
        "    # Usa il file CSV caricato dall'utente\n",
        "    csv_path = os.path.join(os.getcwd(), csv_filename)\n",
        "    download_and_process_data(csv_path, DATA_DIR)\n",
        "\n",
        "# Step 2: Decomposizione wavelet\n",
        "if execute_wavelet:\n",
        "    print(\"\\n====== FASE 2: Decomposizione wavelet ======\")\n",
        "    process_all_signals(DATA_DIR, wavelet='db4', level=6, plot=True)\n",
        "\n",
        "# Step 3: Addestramento del modello CGAN\n",
        "if execute_training:\n",
        "    print(\"\\n====== FASE 3: Addestramento del modello CGAN ======\")\n",
        "    generator, discriminators = train_cgan(\n",
        "        data_dir=DATA_DIR,\n",
        "        output_dir=OUTPUT_DIR,\n",
        "        n_epochs=n_epochs,\n",
        "        batch_size=batch_size,\n",
        "        latent_dim=100,\n",
        "        lr=0.0002,\n",
        "        wavelet_level=6,\n",
        "        save_interval=5\n",
        "    )\n",
        "\n",
        "# Step 4: Generazione e valutazione di accelerogrammi sintetici\n",
        "if execute_generation:\n",
        "    print(\"\\n====== FASE 4: Generazione e valutazione di accelerogrammi sintetici ======\")\n",
        "    model_path = os.path.join(OUTPUT_DIR, \"models\", \"latest_model.pt\")\n",
        "\n",
        "    if os.path.exists(model_path):\n",
        "        # Carica il modello addestrato\n",
        "        generator = load_model(model_path)\n",
        "\n",
        "        # Estrai i valori di magnitudo dai dati reali per usarli come condizioni\n",
        "        try:\n",
        "            df = pd.read_csv(csv_path, sep=';')\n",
        "            magnitudes = df['mw'].unique()\n",
        "\n",
        "            # Crea condizioni basate sui dati reali\n",
        "            conditions = []\n",
        "            for mw in magnitudes:\n",
        "                # Aggiungi diverse combinazioni di PGA per ogni magnitudo\n",
        "                pga_values = [0.1, 0.2, 0.3]  # Valori PGA di esempio\n",
        "                for pga in pga_values:\n",
        "                    conditions.append((float(mw), pga))\n",
        "\n",
        "            # Limita a 10 condizioni per non generare troppi segnali\n",
        "            conditions = conditions[:10]\n",
        "\n",
        "            print(f\"Generazione di accelerogrammi con le seguenti condizioni:\")\n",
        "            for mw, pga in conditions:\n",
        "                print(f\"Mw: {mw}, PGA: {pga}g\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'estrazione delle magnitudo dai dati reali: {str(e)}\")\n",
        "            print(\"Utilizzo di condizioni predefinite...\")\n",
        "\n",
        "            # Condizioni predefinite in caso di errore\n",
        "            conditions = [\n",
        "                (4.5, 0.10),\n",
        "                (5.0, 0.15),\n",
        "                (5.5, 0.20),\n",
        "                (6.0, 0.25),\n",
        "                (6.5, 0.30),\n",
        "                (7.0, 0.40),\n",
        "            ]\n",
        "\n",
        "        # Directory di output per gli accelerogrammi generati\n",
        "        generated_dir = os.path.join(OUTPUT_DIR, \"generated\")\n",
        "        evaluation_dir = os.path.join(OUTPUT_DIR, \"evaluation\")\n",
        "\n",
        "        # Genera gli accelerogrammi\n",
        "        print(\"Generazione accelerogrammi sintetici...\")\n",
        "        synthetic_signals = generate_accelerograms(\n",
        "            generator,\n",
        "            n_samples=len(conditions),\n",
        "            conditions=conditions,\n",
        "            output_dir=generated_dir\n",
        "        )\n",
        "\n",
        "        # Valuta gli accelerogrammi\n",
        "        print(\"Valutazione accelerogrammi sintetici...\")\n",
        "        metrics = evaluate_accelerograms(\n",
        "            synthetic_signals,\n",
        "            output_dir=evaluation_dir\n",
        "        )\n",
        "\n",
        "        print(\"Metriche di valutazione:\")\n",
        "        for key, value in metrics.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "    else:\n",
        "        print(f\"ERRORE: Modello non trovato in {model_path}. Esegui prima l'addestramento.\")\n",
        "\n",
        "print(\"\\nProcesso completato!\")"
      ]
    }
  ]
}